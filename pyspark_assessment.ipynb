{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to import PySpark and check the version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/24 14:22:28 WARN Utils: Your hostname, AI-CJB-LAP-459 resolves to a loopback address: 127.0.1.1; using 192.168.1.164 instead (on interface wlp0s20f3)\n",
      "24/09/24 14:22:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/24 14:22:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.master(\"local[1]\").appName(\"SparkAssessment.com\").getOrCreate()\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id,row_number,min,max\n",
    "from pyspark.sql import Window,functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How to convert the index of a PySpark DataFrame into a column?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|  Alice|    1|\n",
      "|    Bob|    2|\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+-----+\n",
      "|   Name|Value|index|\n",
      "+-------+-----+-----+\n",
      "|  Alice|    1|    0|\n",
      "|    Bob|    2|    1|\n",
      "|Charlie|    3|    2|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/24 14:22:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|   Name|Value|index|\n",
      "+-------+-----+-----+\n",
      "|  Alice|    1|    0|\n",
      "|    Bob|    2|    1|\n",
      "|Charlie|    3|    2|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 1),\n",
    "(\"Bob\", 2),\n",
    "(\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "df.show()\n",
    "df_with_index=df.withColumn(\"index\",monotonically_increasing_id())\n",
    "df_with_index.show()\n",
    "\n",
    "#or\n",
    "window_spec=Window.orderBy(\"value\")\n",
    "df_index=df.withColumn(\"index\",row_number().over(window_spec)-1)\n",
    "df_index.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How to combine many lists to form a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  c|  3|\n",
      "|  d|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]\n",
    "data=list(zip(list1,list2))\n",
    "df=spark.createDataFrame(data,[\"c1\",\"c2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How to get the items of list A not present in list B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "\n",
    "rdd1=spark.sparkContext.parallelize(list_A)\n",
    "rdd2=spark.sparkContext.parallelize(list_B)\n",
    "\n",
    "res=rdd1.subtract(rdd2)\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.How to get the items not common to both list A and list B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3, 6, 8, 7]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1=rdd2.subtract(rdd1)\n",
    "res_union=res.union(res1)\n",
    "res_union.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n",
      "min value: 10\n",
      "25th quartile: 20.0\n",
      "median: 30.0\n",
      "75th quartile: 50.0\n",
      "max val 86\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()\n",
    "min_val=df.agg(min(\"age\")).collect()[0][0]\n",
    "max_val=df.agg(max(\"age\")).collect()[0][0]\n",
    "\n",
    "quantiles=df.approxQuantile(\"age\",[0.25,0.50,0.75],0.01)\n",
    "\n",
    "print(\"min value:\",min_val)\n",
    "print(\"25th quartile:\",quantiles[0])\n",
    "print(\"median:\",quantiles[1])\n",
    "print(\"75th quartile:\",quantiles[2])\n",
    "print(\"max val\",max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How to get frequency counts of unique items of a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|   Doctor|\n",
      "+----+---------+\n",
      "\n",
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "|Scientist|    2|\n",
      "|   Doctor|    1|\n",
      "| Engineer|    4|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()\n",
    "name_count=df.groupBy(\"job\").count()\n",
    "name_count.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|    Other|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_2_jobs = name_count.orderBy(F.desc(\"count\")).limit(2).select(\"job\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Replace other jobs with 'Other'\n",
    "result_df = df.withColumn(\"job\", F.when(df[\"job\"].isin(top_2_jobs), df[\"job\"]).otherwise(\"Other\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. How to Drop rows with NA values specific to a particular column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   B| NULL|123|\n",
      "|   B|    3|456|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n",
      "DataFrame with Renamed Columns:\n",
      "+--------+--------+--------+\n",
      "|new_col1|new_col2|new_col3|\n",
      "+--------+--------+--------+\n",
      "|       1|       2|       3|\n",
      "|       4|       5|       6|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "\n",
    "df.show()\n",
    "\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "print(\"DataFrame with Renamed Columns:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. How to bin a numeric list to 10 groups of equal size?\n",
    "CONFUSING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|             values|Buckets|\n",
      "+-------------------+-------+\n",
      "|  0.619189370225301|    0.0|\n",
      "| 0.5096018842446481|    0.0|\n",
      "| 0.8325259388871524|    0.0|\n",
      "|0.26322809041172357|    0.0|\n",
      "| 0.6702867696264135|    0.0|\n",
      "+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+\n",
      "|             values|\n",
      "+-------------------+\n",
      "|  0.619189370225301|\n",
      "| 0.5096018842446481|\n",
      "| 0.8325259388871524|\n",
      "|0.26322809041172357|\n",
      "| 0.6702867696264135|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand,initcap\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
    "num_items = 100\n",
    "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n",
    "bucketizer=Bucketizer(splits=[0,10,float(\"Inf\")],inputCol=\"values\",outputCol=\"Buckets\")\n",
    "df_buck=bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "df_buck.show(5)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. How to create contigency table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+---+\n",
      "|category1_category2|  X|  Y|\n",
      "+-------------------+---+---+\n",
      "|                  A|  2|  1|\n",
      "|                  B|  1|  1|\n",
      "|                  C|  2|  1|\n",
      "+-------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
    "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
    "df.crosstab(\"category1\",\"category2\").sort(\"category1_category2\").show()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. How to find the numbers that are multiples of 3 from a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|random|\n",
      "+---+------+\n",
      "|  1|     6|\n",
      "|  2|     9|\n",
      "|  3|     3|\n",
      "|  5|     6|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand,col\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "mul_3=df.filter(col(\"random\")%3==0)\n",
    "mul_3.show()\n",
    "# Show the DataFrame\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. How to extract items at given positions from a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|random|\n",
      "+---+------+\n",
      "|  0|     7|\n",
      "|  1|     6|\n",
      "|  2|     9|\n",
      "|  3|     3|\n",
      "|  4|     7|\n",
      "|  5|     6|\n",
      "|  6|    10|\n",
      "|  7|     1|\n",
      "|  8|    10|\n",
      "|  9|     8|\n",
      "+---+------+\n",
      "\n",
      "[7, 7, 10, 6]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "pos = [0, 4, 8, 5]\n",
    "\n",
    "item=df.select(\"random\").collect()\n",
    "res=[item[i][0] for i in pos]\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. How to stack two DataFrames vertically ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "+------+-----+-----+\n",
      "\n",
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_3|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n",
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for region A\n",
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_A.show()\n",
    "\n",
    "# Create DataFrame for region B\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "df_B.show()\n",
    "\n",
    "res_df=df_A.union(df_B)\n",
    "\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. How to compute the mean squared error on a truth and predicted columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|actual|predicted|\n",
      "+------+---------+\n",
      "|     1|        1|\n",
      "|     2|        4|\n",
      "|     3|        9|\n",
      "|     4|       16|\n",
      "|     5|       25|\n",
      "+------+---------+\n",
      "\n",
      "116.8\n"
     ]
    }
   ],
   "source": [
    "# Assume you have a DataFrame df with two columns \"actual\" and \"predicted\"\n",
    "# For the sake of example, we'll create a sample DataFrame\n",
    "from pyspark.sql.functions import mean, col\n",
    "\n",
    "\n",
    "data = [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
    "df = spark.createDataFrame(data, [\"actual\", \"predicted\"])\n",
    "\n",
    "df.show()\n",
    "squared_error=df.withColumn(\"mean-squared-error\",(col(\"actual\")-col(\"predicted\"))**2)\n",
    "mse=squared_error.select(mean(\"mean-squared-error\")).collect()[0][0]\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. How to convert the first character of each element in a series to uppercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| John|\n",
      "|Alice|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| john|\n",
      "|alice|\n",
      "|  bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have the following DataFrame\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "df.withColumn(\"name\",initcap(df[\"name\"])).show()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. How to compute summary statistics for all columns in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+-----------------+\n",
      "|summary|  name|              age|           salary|\n",
      "+-------+------+-----------------+-----------------+\n",
      "|  count|     5|                5|                5|\n",
      "|   mean|  NULL|             32.4|          66000.0|\n",
      "| stddev|  NULL|3.209361307176242|9617.692030835673|\n",
      "|    min| James|               29|            55000|\n",
      "|    max|Robert|               37|            80000|\n",
      "+-------+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For the sake of example, we'll create a sample DataFrame\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "Describe=df.describe()\n",
    "Describe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. How to calculate the number of characters in each word in a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, explode,split,lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|length|\n",
      "+-----+------+\n",
      "| john|     4|\n",
      "|alice|     5|\n",
      "|  bob|     3|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have the following DataFrame\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "# df.show()\n",
    "df.withColumn(\"length\",length(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 How to compute difference of differences between consecutive numbers of a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  James| 34| 55000|\n",
      "|Michael| 30| 70000|\n",
      "| Robert| 37| 60000|\n",
      "|  Maria| 29| 80000|\n",
      "|    Jen| 32| 65000|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/24 14:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+------------+\n",
      "|   name|salary|salary_diff|diff_of_diff|\n",
      "+-------+------+-----------+------------+\n",
      "|  James| 55000|       NULL|        NULL|\n",
      "| Robert| 60000|       5000|        NULL|\n",
      "|    Jen| 65000|       5000|           0|\n",
      "|Michael| 70000|       5000|           0|\n",
      "|  Maria| 80000|      10000|        5000|\n",
      "+-------+------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For the sake of example, we'll create a sample DataFrame\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()\n",
    "window_spec = Window.orderBy(\"salary\")\n",
    "df_with_diff = df.withColumn(\"salary_diff\", col(\"salary\") - lag(\"salary\").over(window_spec))\n",
    "\n",
    "df_with_diff_of_diff = df_with_diff.withColumn(\"diff_of_diff\", col(\"salary_diff\") - lag(\"salary_diff\").over(window_spec))\n",
    "\n",
    "df_with_diff_of_diff.select(\"name\", \"salary\", \"salary_diff\", \"diff_of_diff\").show()\n",
    "\n",
    "# window_spec=Window.orderBy(\"salary\")\n",
    "# salary_diff=df.withColumn(\"salary_diff\",col(\"salary\")-lag(\"salary\").over(window_spec))\n",
    "# salary_diff_diff=salary_diff.withColumn(\"salary_diff_diff\",col(\"salary_diff\")-lag(\"salary_diff\").over(window_spec))\n",
    "# df.select(\"name\",\"age\",\"salary\",\"salary_diff\",\"salary_diff_diff\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. How to get the day of month, week number, day of year and day of week from a date strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|date_str_1| date_str_2|\n",
      "+----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|\n",
      "|2023-12-31|01 Jan 2010|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+-----------+--------------+-------------+-------------+-------------+--------------+-------------+-------------+-------------+\n",
      "|date_str_1| date_str_2|day_of_month_1|week_number_1|day_of_year_1|day_of_week_1|day_of_month_2|week_number_2|day_of_year_2|day_of_week_2|\n",
      "+----------+-----------+--------------+-------------+-------------+-------------+--------------+-------------+-------------+-------------+\n",
      "|2023-05-18|01 Jan 2010|            18|           20|          138|            5|             1|           53|            1|            6|\n",
      "|2023-12-31|01 Jan 2010|            31|           52|          365|            1|             1|           53|            1|            6|\n",
      "+----------+-----------+--------------+-------------+-------------+-------------+--------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df.show()\n",
    "df_with_dates = df.select(\n",
    "    col(\"date_str_1\"),\n",
    "    col(\"date_str_2\"),\n",
    "    to_date(col(\"date_str_1\")).alias(\"date_1\"),\n",
    "    to_date(col(\"date_str_2\"), \"dd MMM yyyy\").alias(\"date_2\")\n",
    ")\n",
    "df_final = df_with_dates.select(\n",
    "    \"date_str_1\",\n",
    "    \"date_str_2\",\n",
    "    dayofmonth(\"date_1\").alias(\"day_of_month_1\"),\n",
    "    weekofyear(\"date_1\").alias(\"week_number_1\"),\n",
    "    dayofyear(\"date_1\").alias(\"day_of_year_1\"),\n",
    "    dayofweek(\"date_1\").alias(\"day_of_week_1\"),\n",
    "    dayofmonth(\"date_2\").alias(\"day_of_month_2\"),\n",
    "    weekofyear(\"date_2\").alias(\"week_number_2\"),\n",
    "    dayofyear(\"date_2\").alias(\"day_of_year_2\"),\n",
    "    dayofweek(\"date_2\").alias(\"day_of_week_2\")\n",
    ")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. How to convert year-month string to dates corresponding to the 4th day of the month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|MonthYear|\n",
      "+---------+\n",
      "| Jan 2010|\n",
      "| Feb 2011|\n",
      "| Mar 2012|\n",
      "+---------+\n",
      "\n",
      "+---------+----------+\n",
      "|MonthYear|      date|\n",
      "+---------+----------+\n",
      "| Jan 2010|2010-01-04|\n",
      "| Feb 2011|2011-02-04|\n",
      "| Mar 2012|2012-03-04|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example dataframe\n",
    "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "\n",
    "df.show()\n",
    "df.withColumn(\n",
    "    \"date\",\n",
    "    to_date(concat(col(\"MonthYear\"),lit(\" 4\")),\"MMM yyyy d\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23 How to filter words that contain atleast 2 vowels from a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "| Apple|\n",
      "|Orange|\n",
      "|  Plan|\n",
      "|Python|\n",
      "| Money|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "| Apple|\n",
      "|Orange|\n",
      "|  Plan|\n",
      "|Python|\n",
      "| Money|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example dataframe\n",
    "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
    "\n",
    "df.show()\n",
    "df.filter(col(\"word\").rlike(\"[aeiouAEIOU]\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. How to filter valid emails from a list?\n",
    "Difficulty Level: L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|email                     |\n",
      "+--------------------------+\n",
      "|buying books at amazom.com|\n",
      "|rameses@egypt.com         |\n",
      "|matt@t.co                 |\n",
      "|narendra@modi.com         |\n",
      "+--------------------------+\n",
      "\n",
      "+-----------------+\n",
      "|            email|\n",
      "+-----------------+\n",
      "|rameses@egypt.com|\n",
      "|        matt@t.co|\n",
      "|narendra@modi.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a list\n",
    "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
    "\n",
    "# Convert the list to DataFrame\n",
    "df = spark.createDataFrame(data, \"string\").toDF(\"email\")\n",
    "df.show(truncate =False)\n",
    "df.filter(col(\"email\").rlike(\"@[a-zA-Z0-9.-]\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. How to Pivot PySpark DataFrame?\n",
    "Convert region categories to Columns and sum the revenue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+\n",
      "|year|quarter|region|revenue|\n",
      "+----+-------+------+-------+\n",
      "|2021|      1|    US|   5000|\n",
      "|2021|      1|    EU|   4000|\n",
      "|2021|      2|    US|   5500|\n",
      "|2021|      2|    EU|   4500|\n",
      "|2021|      3|    US|   6000|\n",
      "|2021|      3|    EU|   5000|\n",
      "|2021|      4|    US|   7000|\n",
      "|2021|      4|    EU|   6000|\n",
      "+----+-------+------+-------+\n",
      "\n",
      "+------+------------+\n",
      "|region|sum(revenue)|\n",
      "+------+------------+\n",
      "|    US|       23500|\n",
      "|    EU|       19500|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample data\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.groupBy(\"region\").sum(\"revenue\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. How to get the mean of a variable grouped by another variable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|OrderID|   Product|Price|\n",
      "+-------+----------+-----+\n",
      "|   1001|    Laptop| 1000|\n",
      "|   1002|     Mouse|   50|\n",
      "|   1003|    Laptop| 1200|\n",
      "|   1004|     Mouse|   30|\n",
      "|   1005|Smartphone|  700|\n",
      "+-------+----------+-----+\n",
      "\n",
      "+----------+----------+\n",
      "|   product|avg(price)|\n",
      "+----------+----------+\n",
      "|    Laptop|    1100.0|\n",
      "|     Mouse|      40.0|\n",
      "|Smartphone|     700.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample data\n",
    "data = [(\"1001\", \"Laptop\", 1000),\n",
    "(\"1002\", \"Mouse\", 50),\n",
    "(\"1003\", \"Laptop\", 1200),\n",
    "(\"1004\", \"Mouse\", 30),\n",
    "(\"1005\", \"Smartphone\", 700)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"OrderID\", \"Product\", \"Price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "df.groupBy(\"product\").mean(\"price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. How to compute the euclidean distance between two columns?\n",
    "\n",
    "Compute the euclidean distance between series (points) p and q, without using a packaged formula.\n",
    "\n",
    "NEWLY LEARNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|series1|series2|\n",
      "+-------+-------+\n",
      "|      1|     10|\n",
      "|      2|      9|\n",
      "|      3|      8|\n",
      "|      4|      7|\n",
      "|      5|      6|\n",
      "|      6|      5|\n",
      "|      7|      4|\n",
      "|      8|      3|\n",
      "|      9|      2|\n",
      "|     10|      1|\n",
      "+-------+-------+\n",
      "\n",
      "+------------------+\n",
      "|euclidean_distance|\n",
      "+------------------+\n",
      "| 18.16590212458495|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define your series\n",
    "data = [(1, 10), (2, 9), (3, 8), (4, 7), (5, 6), (6, 5), (7, 4), (8, 3), (9, 2), (10, 1)]\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = spark.createDataFrame(data, [\"series1\", \"series2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"series1\", \"series2\"], outputCol=\"vectors\")\n",
    "df = vecAssembler.transform(df)\n",
    "\n",
    "df = df.withColumn(\"squared_diff\", expr(\"POW(series1 - series2, 2)\"))\n",
    "\n",
    "df.agg(expr(\"SQRT(SUM(squared_diff))\").alias(\"euclidean_distance\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. How to replace missing spaces in a string with the least frequent character?\n",
    "NEWLY LEARNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|           string|\n",
      "+-----------------+\n",
      "|dbc deb abed gade|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+-----------------+\n",
      "|           string|  modified_string|\n",
      "+-----------------+-----------------+\n",
      "|dbc deb abed gade|dbcgdebgabedggade|\n",
      "+-----------------+-----------------+\n",
      "\n",
      "Row(character='g', count=1)\n"
     ]
    }
   ],
   "source": [
    "#Sample DataFrame\n",
    "df = spark.createDataFrame([('dbc deb abed gade',),], [\"string\"])\n",
    "df.show()\n",
    "char_df=df.select(explode(split(col(\"string\"),\"\")).alias(\"character\"))\n",
    "c_count=char_df.groupBy(\"character\").count()\n",
    "leaast_f_count=c_count.orderBy(\"count\").first()\n",
    "least_char=leaast_f_count[\"character\"]\n",
    "df.withColumn(\"modified_string\",regexp_replace(col(\"string\"),\" \",least_char)).show()\n",
    "print(leaast_f_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. How to create a TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      date|               value|\n",
      "+----------+--------------------+\n",
      "|2000-01-01| 0.28963143741864994|\n",
      "|2000-01-08|0.010109359047473232|\n",
      "|2000-01-15|  0.1826239519326598|\n",
      "|2000-01-22| 0.49933749531327565|\n",
      "|2000-01-29|  0.6701840518611715|\n",
      "|2000-02-05| 0.15659396376977308|\n",
      "|2000-02-12| 0.19577051852671057|\n",
      "|2000-02-19|  0.5489449081993252|\n",
      "|2000-02-26|  0.9551001799727961|\n",
      "|2000-03-04|  0.9391057608934886|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_date = '2000-01-01'\n",
    "num_saturdays = 10\n",
    "\n",
    "# Generate the list of Saturdays\n",
    "saturdays = spark.range(num_saturdays).select(\n",
    "    date_add(expr(f\"to_date('{start_date}')\"), (7 * col(\"id\")).cast(\"int\")).alias(\"date\")\n",
    ")\n",
    "saturdays.withColumn(\"value\",rand()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. How to get the nrows, ncolumns, datatype of a dataframe?\n",
    "\n",
    "Get the number of rows, columns, datatype and summary statistics of each column of the Churn_Modelling dataset. Also get the numpy array and list equivalent of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|RowNumber|CustomerId|Surname |CreditScore|Geography|Gender|Age|Tenure|Balance  |NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|1        |15634602  |Hargrave|619        |France   |Female|42 |2     |0.0      |1            |1        |1             |101348.88      |1     |\n",
      "|2        |15647311  |Hill    |608        |Spain    |Female|41 |1     |83807.86 |1            |0        |1             |112542.58      |0     |\n",
      "|3        |15619304  |Onio    |502        |France   |Female|42 |8     |159660.8 |3            |1        |0             |113931.57      |1     |\n",
      "|4        |15701354  |Boni    |699        |France   |Female|39 |1     |0.0      |2            |0        |0             |93826.63       |0     |\n",
      "|5        |15737888  |Mitchell|850        |Spain    |Female|43 |2     |125510.82|1            |1        |1             |79084.1        |0     |\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "nrows: 10000\n",
      "nColumns: 14\n",
      "datatype: {'RowNumber': IntegerType(), 'CustomerId': IntegerType(), 'Surname': StringType(), 'CreditScore': IntegerType(), 'Geography': StringType(), 'Gender': StringType(), 'Age': IntegerType(), 'Tenure': IntegerType(), 'Balance': DoubleType(), 'NumOfProducts': IntegerType(), 'HasCrCard': IntegerType(), 'IsActiveMember': IntegerType(), 'EstimatedSalary': DoubleType(), 'Exited': IntegerType()}\n",
      "numpy array shape:  (10000, 14)\n",
      "summary statistics: DataFrame[summary: string, RowNumber: string, CustomerId: string, Surname: string, CreditScore: string, Geography: string, Gender: string, Age: string, Tenure: string, Balance: string, NumOfProducts: string, HasCrCard: string, IsActiveMember: string, EstimatedSalary: string, Exited: string]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling.csv\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"Churn_Modelling.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "#df = spark.read.csv(\"C:/Users/RajeshVaddi/Documents/MLPlus/DataSets/Churn_Modelling.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "nrows=df.count()\n",
    "print(\"nrows:\",nrows)\n",
    "ncolumns=len(df.columns)\n",
    "print(\"nColumns:\",ncolumns)\n",
    "\n",
    "data_types={col:(df.schema[col].dataType)for col in df.columns}\n",
    "print(\"datatype:\",data_types)\n",
    "summary_stat=df.describe()\n",
    "numpy_arr=df.toPandas().to_numpy()\n",
    "print(\"numpy array shape: \",numpy_arr.shape)\n",
    "print(\"summary statistics:\",summary_stat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. How to rename a specific columns in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "| name|age|qty|\n",
      "+-----+---+---+\n",
      "|Alice|  1| 30|\n",
      "|  Bob|  2| 35|\n",
      "+-----+---+---+\n",
      "\n",
      "+-----+--------+--------+\n",
      "| name|user_age|user_qty|\n",
      "+-----+--------+--------+\n",
      "|Alice|       1|      30|\n",
      "|  Bob|       2|      35|\n",
      "+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('Alice', 1, 30),('Bob', 2, 35)], [\"name\", \"age\", \"qty\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Rename lists for specific columns\n",
    "old_names = [\"qty\", \"age\"]\n",
    "new_names = [\"user_qty\", \"user_age\"]\n",
    "for old_name,new_name in zip(old_names,new_names):\n",
    "    \n",
    "    df=df.withColumnRenamed(old_name,new_name)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. How to check if a dataframe has any missing values and count of missing values in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n",
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   0|    2|  2|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "null_count=df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_count.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33 How to replace missing values of multiple numeric columns with the mean?\n",
    "\n",
    "MAKE USE OF IMPUTER FUNCTION RATHER USING FILL OR FILLNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1|NULL|\n",
      "|   B|NULL| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6|NULL|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1| 289|\n",
      "|   B|   3| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6| 289|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, 123 ),\n",
    "(\"B\", 3, 456),\n",
    "(\"D\", 6, None),\n",
    "], [\"Name\", \"var1\", \"var2\"])\n",
    "\n",
    "df.show()\n",
    "cols=[\"var1\",\"var2\"]\n",
    "imputer=Imputer(inputCols=cols,outputCols=cols,strategy=\"mean\")\n",
    "model=imputer.fit(df)\n",
    "imputed_df=model.transform(df)\n",
    "imputed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34. How to change the order of columns of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|First_Name|Last_Name|Age|\n",
      "+----------+---------+---+\n",
      "|      John|      Doe| 30|\n",
      "|      Jane|      Doe| 25|\n",
      "|     Alice|    Smith| 22|\n",
      "+----------+---------+---+\n",
      "\n",
      "+---+----------+---------+\n",
      "|Age|First_Name|Last_Name|\n",
      "+---+----------+---------+\n",
      "| 30|      John|      Doe|\n",
      "| 25|      Jane|      Doe|\n",
      "| 22|     Alice|    Smith|\n",
      "+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Doe\", 25), (\"Alice\", \"Smith\", 22)]\n",
    "\n",
    "# Create DataFrame from the data\n",
    "df = spark.createDataFrame(data, [\"First_Name\", \"Last_Name\", \"Age\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "new_order=[\"Age\",\"First_Name\",\"Last_Name\"]\n",
    "df.select(*new_order).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. How to format or suppress scientific notations in a PySpark DataFrame?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|your_column|\n",
      "+---+-----------+\n",
      "|  1|    1.23E-7|\n",
      "|  2|  2.3456E-5|\n",
      "|  3| 3.45678E-4|\n",
      "+---+-----------+\n",
      "\n",
      "+---+------------+\n",
      "| id| your_column|\n",
      "+---+------------+\n",
      "|  1|0.0000001230|\n",
      "|  2|0.0000234560|\n",
      "|  3|0.0003456780|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame df and the column you want to format is 'your_column'\n",
    "df = spark.createDataFrame([(1, 0.000000123), (2, 0.000023456), (3, 0.000345678)], [\"id\", \"your_column\"])\n",
    "\n",
    "df.show()\n",
    "df.withColumn(\"your_column\",format_number(\"your_column\",10)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "36. How to format all the values in a dataframe as percentages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|numbers_1|numbers_2|\n",
      "+---------+---------+\n",
      "|      0.1|     0.08|\n",
      "|      0.2|     0.06|\n",
      "|     0.33|     0.02|\n",
      "+---------+---------+\n",
      "\n",
      "+---------+---------+\n",
      "|numbers_1|numbers_2|\n",
      "+---------+---------+\n",
      "|   10.00%|    8.00%|\n",
      "|   20.00%|    6.00%|\n",
      "|   33.00%|    2.00%|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(0.1, .08), (0.2, .06), (0.33, .02)]\n",
    "df = spark.createDataFrame(data, [\"numbers_1\", \"numbers_2\"])\n",
    "\n",
    "df.show()\n",
    "# df.withColumn(\"numbers_2\",format_number(\"numbers_2\",100)).show()\n",
    "df_percentage = df.select(\n",
    "    [F.concat(F.format_number(F.col(col_name) * 100, 2), F.lit('%')).alias(col_name) for col_name in df.columns]\n",
    ")\n",
    "df_percentage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "37. How to filter every nth row in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   Name|Number|\n",
      "+-------+------+\n",
      "|  Alice|     1|\n",
      "|    Bob|     2|\n",
      "|Charlie|     3|\n",
      "|   Dave|     4|\n",
      "|    Eve|     5|\n",
      "|  Frank|     6|\n",
      "|  Grace|     7|\n",
      "| Hannah|     8|\n",
      "|   Igor|     9|\n",
      "|   Jack|    10|\n",
      "+-------+------+\n",
      "\n",
      "+----+------+---+\n",
      "|Name|Number| rn|\n",
      "+----+------+---+\n",
      "| Eve|     5|  5|\n",
      "|Jack|    10| 10|\n",
      "+----+------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/24 14:22:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3), (\"Dave\", 4), (\"Eve\", 5),\n",
    "(\"Frank\", 6), (\"Grace\", 7), (\"Hannah\", 8), (\"Igor\", 9), (\"Jack\", 10)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Number\"])\n",
    "\n",
    "df.show()\n",
    "window_spec=Window.orderBy(monotonically_increasing_id())\n",
    "df=df.withColumn(\"rn\",row_number().over(window_spec))\n",
    "\n",
    "n=5\n",
    "\n",
    "df=df.filter((df.rn%n)==0)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38 How to get the row number of the nth largest value in a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|column1|\n",
      "+---+-------+\n",
      "|  1|      5|\n",
      "|  2|      8|\n",
      "|  3|     12|\n",
      "|  4|      1|\n",
      "|  5|     15|\n",
      "|  6|      7|\n",
      "+---+-------+\n",
      "\n",
      "Row number: 3\n",
      "Column value: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/24 14:22:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 14:22:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "Row(id=1, column1=5),\n",
    "Row(id=2, column1=8),\n",
    "Row(id=3, column1=12),\n",
    "Row(id=4, column1=1),\n",
    "Row(id=5, column1=15),\n",
    "Row(id=6, column1=7),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, row_number\n",
    "\n",
    "window = Window.orderBy(desc(\"column1\"))\n",
    "df = df.withColumn(\"row_number\", row_number().over(window))\n",
    "\n",
    "n = 3 # We're interested in the 3rd largest value.\n",
    "row = df.filter(df.row_number == n).first()\n",
    "\n",
    "if row:\n",
    "    print(\"Row number:\", row.row_number)\n",
    "print(\"Column value:\", row.column1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "39. How to get the last n rows of a dataframe with row sum > 100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|  10|  25|  70|\n",
      "|  40|   5|  20|\n",
      "|  70|  80| 100|\n",
      "|  10|   2|  60|\n",
      "|  40|  50|  20|\n",
      "+----+----+----+\n",
      "\n",
      "Row(col1=10, col2=25, col3=70, row_sum=105)\n",
      "Row(col1=70, col2=80, col3=100, row_sum=250)\n",
      "Row(col1=40, col2=50, col3=20, row_sum=110)\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(10, 25, 70),\n",
    "(40, 5, 20),\n",
    "(70, 80, 100),\n",
    "(10, 2, 60),\n",
    "(40, 50, 20)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# Display original DataFrame\n",
    "df.show()\n",
    "df_sum=df.withColumn(\"row_sum\",F.col(\"col1\")+F.col(\"col2\")+F.col(\"col3\"))\n",
    "filtered_df=df_sum.filter(F.col(\"row_sum\")>100)\n",
    "n=5\n",
    "last_n_rows=filtered_df.tail(n)\n",
    "for row in last_n_rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40. How to create a column containing the minimum by maximum of each row?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf,array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "|   7|   8|   9|\n",
      "|  10|  11|  12|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+-------+-------+------------------+\n",
      "|col1|col2|col3|row_min|row_max|        min_by_max|\n",
      "+----+----+----+-------+-------+------------------+\n",
      "|   1|   2|   3|      1|      3|0.3333333333333333|\n",
      "|   4|   5|   6|      4|      6|0.6666666666666666|\n",
      "|   7|   8|   9|      7|      9|0.7777777777777778|\n",
      "|  10|  11|  12|     10|     12|0.8333333333333334|\n",
      "+----+----+----+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample Data\n",
    "data = [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "df.show()\n",
    "# max_df=df.withColumn(\"maximum_no\",F.greatest(\"col1\",\"col2\",\"col3\"))\n",
    "# min_of_max=max_df.select(F.min(\"maximum_no\")).collect()[0][0]\n",
    "# print(\"minimum of maximum by each row is:\",min_of_max)\n",
    "# max_df.show()\n",
    "min_max_df=df.withColumn(\"row_min\",F.least(\"col1\",\"col2\",\"col3\")) \\\n",
    "    .withColumn(\"row_max\",F.greatest(\"col1\",\"col2\",\"col3\"))\n",
    "result_df=min_max_df.withColumn(\"min_by_max\",F.col(\"row_min\")/F.col(\"row_max\"))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41. How to create a column that contains the penultimate value in each row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|Column1|Column2|Column3|\n",
      "+-------+-------+-------+\n",
      "|     10|     20|     30|\n",
      "|     40|     60|     50|\n",
      "|     80|     70|     90|\n",
      "+-------+-------+-------+\n",
      "\n",
      "+-------+-------+-------+-----------+\n",
      "|Column1|Column2|Column3|penultimate|\n",
      "+-------+-------+-------+-----------+\n",
      "|     10|     20|     30|         20|\n",
      "|     40|     60|     50|         50|\n",
      "|     80|     70|     90|         80|\n",
      "+-------+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(10, 20, 30),\n",
    "(40, 60, 50),\n",
    "(80, 70, 90)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, IntegerType,DoubleType\n",
    "\n",
    "# Define UDF to sort array in descending order\n",
    "def sort_row_get_second(row):\n",
    "    return sorted(row,reverse=True)[1]\n",
    "sort_row_get_second_udf=udf(sort_row_get_second,IntegerType())\n",
    "df_with_second=df.withColumn(\"penultimate\",sort_row_get_second_udf(array(\"Column1\", \"Column2\", \"Column3\")))\n",
    "df_with_second.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42. How to normalize all columns in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Col1|Col2|Col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   2|   3|   4|\n",
      "|   3|   4|   5|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+-------------+\n",
      "|Col1|Col2|Col3|     features|\n",
      "+----+----+----+-------------+\n",
      "|   1|   2|   3|[1.0,2.0,3.0]|\n",
      "|   2|   3|   4|[2.0,3.0,4.0]|\n",
      "|   3|   4|   5|[3.0,4.0,5.0]|\n",
      "|   4|   5|   6|[4.0,5.0,6.0]|\n",
      "+----+----+----+-------------+\n",
      "\n",
      "+----+----+----+-------------+--------------------+\n",
      "|Col1|Col2|Col3|     features|     scaled_features|\n",
      "+----+----+----+-------------+--------------------+\n",
      "|   1|   2|   3|[1.0,2.0,3.0]|[-1.1618950038622...|\n",
      "|   2|   3|   4|[2.0,3.0,4.0]|[-0.3872983346207...|\n",
      "|   3|   4|   5|[3.0,4.0,5.0]|[0.38729833462074...|\n",
      "|   4|   5|   6|[4.0,5.0,6.0]|[1.16189500386222...|\n",
      "+----+----+----+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a sample dataframe\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(3, 4, 5),\n",
    "(4, 5, 6)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Col1\", \"Col2\", \"Col3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# define the list of columns to be normalized\n",
    "input_cols = [\"Col1\", \"Col2\", \"Col3\"]\n",
    "\n",
    "# initialize VectorAssembler with input and output column names\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "# transform the data\n",
    "df_assembled = assembler.transform(df)\n",
    "df_assembled.show()\n",
    "# initialize StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# fit and transform the data\n",
    "scalerModel = scaler.fit(df_assembled)\n",
    "df_normalized = scalerModel.transform(df_assembled)\n",
    "df_normalized.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "43. How to get the positions where values of two columns match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Name1|Name2|\n",
      "+-----+-----+\n",
      "| John| John|\n",
      "| Lily| Lucy|\n",
      "|  Sam|  Sam|\n",
      "| Lucy| Lily|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+--------+\n",
      "|Name1|Name2|position|\n",
      "+-----+-----+--------+\n",
      "| John| John|    true|\n",
      "| Lily| Lucy|   false|\n",
      "|  Sam|  Sam|    true|\n",
      "| Lucy| Lily|   false|\n",
      "+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample DataFrame\n",
    "data = [(\"John\", \"John\"), (\"Lily\", \"Lucy\"), (\"Sam\", \"Sam\"), (\"Lucy\", \"Lily\")]\n",
    "df = spark.createDataFrame(data, [\"Name1\", \"Name2\"])\n",
    "\n",
    "df.show()\n",
    "filtered_df=df.withColumn(\"position\",when (col(\"Name1\")==col(\"Name2\"),True).otherwise(False)\n",
    ")\n",
    "\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "44. How to create lags and leads of a column by group in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|      Date| Store|Sales|\n",
      "+----------+------+-----+\n",
      "|2023-01-01|Store1|  100|\n",
      "|2023-01-02|Store1|  150|\n",
      "|2023-01-03|Store1|  200|\n",
      "|2023-01-04|Store1|  250|\n",
      "|2023-01-05|Store1|  300|\n",
      "|2023-01-01|Store2|   50|\n",
      "|2023-01-02|Store2|   60|\n",
      "|2023-01-03|Store2|   80|\n",
      "|2023-01-04|Store2|   90|\n",
      "|2023-01-05|Store2|  120|\n",
      "+----------+------+-----+\n",
      "\n",
      "+----------+------+-----+----+----+\n",
      "|      Date| Store|Sales| lag|lead|\n",
      "+----------+------+-----+----+----+\n",
      "|2023-01-01|Store1|  100|NULL| 200|\n",
      "|2023-01-02|Store1|  150|NULL| 250|\n",
      "|2023-01-03|Store1|  200| 100| 300|\n",
      "|2023-01-04|Store1|  250| 150|NULL|\n",
      "|2023-01-05|Store1|  300| 200|NULL|\n",
      "|2023-01-01|Store2|   50|NULL|  80|\n",
      "|2023-01-02|Store2|   60|NULL|  90|\n",
      "|2023-01-03|Store2|   80|  50| 120|\n",
      "|2023-01-04|Store2|   90|  60|NULL|\n",
      "|2023-01-05|Store2|  120|  80|NULL|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"2023-01-01\", \"Store1\", 100),\n",
    "(\"2023-01-02\", \"Store1\", 150),\n",
    "(\"2023-01-03\", \"Store1\", 200),\n",
    "(\"2023-01-04\", \"Store1\", 250),\n",
    "(\"2023-01-05\", \"Store1\", 300),\n",
    "(\"2023-01-01\", \"Store2\", 50),\n",
    "(\"2023-01-02\", \"Store2\", 60),\n",
    "(\"2023-01-03\", \"Store2\", 80),\n",
    "(\"2023-01-04\", \"Store2\", 90),\n",
    "(\"2023-01-05\", \"Store2\", 120)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Date\", \"Store\", \"Sales\"])\n",
    "\n",
    "df.show()\n",
    "windowspec=Window.partitionBy(\"Store\").orderBy(\"Date\")\n",
    "\n",
    "new_df=df.withColumn(\"lag\",lag(\"Sales\",2).over(windowspec))\\\n",
    "    .withColumn(\"lead\",lead(\"Sales\",2).over(windowspec))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45. How to get the frequency of unique values in the entire dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Value|count|\n",
      "+-----+-----+\n",
      "|    1|    2|\n",
      "|    2|    4|\n",
      "|    4|    3|\n",
      "|    5|    1|\n",
      "|    3|    4|\n",
      "|    6|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(1, 2, 3),\n",
    "(4, 5, 6),\n",
    "(2, 3, 4)]\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "# # Print DataFrame\n",
    "# df.show()\n",
    "# col1_count=df.groupBy(\"Column1\").count()\n",
    "# col2_count=df.groupBy(\"Column2\").count()\n",
    "# col3_count=df.groupBy(\"Column3\").count()\n",
    "# col1_count.show()\n",
    "# col2_count.show()\n",
    "# col3_count.show()\n",
    "\n",
    "#or\n",
    "\n",
    "combined_df=df.selectExpr(\"Column1 as Value\").union(df.selectExpr(\"Column2 as value\")).union(df.selectExpr(\"Column3 as Value\"))\n",
    "value_count=combined_df.groupBy(\"Value\").count()\n",
    "value_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46. How to replace both the diagonals of dataframe with 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    1|    2|    3|    4|\n",
      "|    2|    3|    4|    5|\n",
      "|    1|    2|    3|    4|\n",
      "|    4|    5|    6|    7|\n",
      "+-----+-----+-----+-----+\n",
      "\n",
      "DataFrame after replacing both diagonal elements with 0:\n",
      "+-----+-----+-----+-----+\n",
      "|Col_1|Col_2|Col_3|Col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    0|    2|    3|    0|\n",
      "|    2|    0|    0|    5|\n",
      "|    1|    0|    0|    4|\n",
      "|    0|    5|    6|    0|\n",
      "+-----+-----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/24 17:00:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 17:00:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 17:00:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 17:00:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 17:00:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(1, 2, 3, 4),\n",
    "(4, 5, 6, 7)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "n = len(data)\n",
    "\n",
    "# Replace both diagonal elements with 0\n",
    "for i in range(n):\n",
    "    df = df.withColumn(\"Col_\" + str(i + 1), \n",
    "                       F.when(F.row_number().over(Window.orderBy(F.lit(1))) - 1 == i, 0)  # Main diagonal condition\n",
    "                       .when(F.row_number().over(Window.orderBy(F.lit(1))) - 1 == n - 1 - i, 0) # Anti-diagonal condition\n",
    "                       .otherwise(F.col(\"Col_\" + str(i + 1))))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "print(\"DataFrame after replacing both diagonal elements with 0:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47. How to reverse the rows of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    4|    5|    6|    7|\n",
      "|    3|    4|    5|    6|\n",
      "|    2|    3|    4|    5|\n",
      "|    1|    2|    3|    4|\n",
      "+-----+-----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/24 17:14:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 17:14:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 17:14:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 17:14:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/24 17:14:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(3, 4, 5, 6),\n",
    "(4, 5, 6, 7)]\n",
    "# by directly affecting the input \n",
    "# data=data[::-1]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "# data=data[::-1]\n",
    "\n",
    "# reversing the dataframe without changing the input\n",
    "w=Window.orderBy(monotonically_increasing_id())\n",
    "df=df.withColumn(\"id\",row_number().over(w)-1)\n",
    "df2=df.orderBy(\"id\",ascending=False).drop(\"id\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48. How to create one-hot encodings of a categorical variable (dummy variables)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------------+\n",
      "|Categories|Value|category_numeric|\n",
      "+----------+-----+----------------+\n",
      "|         A|   10|             1.0|\n",
      "|         A|   20|             1.0|\n",
      "|         B|   30|             0.0|\n",
      "|         B|   20|             0.0|\n",
      "|         B|   30|             0.0|\n",
      "|         C|   40|             2.0|\n",
      "|         C|   10|             2.0|\n",
      "|         D|   10|             3.0|\n",
      "+----------+-----+----------------+\n",
      "\n",
      "+----------+-----+----------------+----------------+\n",
      "|Categories|Value|category_numeric|category_encoded|\n",
      "+----------+-----+----------------+----------------+\n",
      "|         A|   10|             1.0|   (3,[1],[1.0])|\n",
      "|         A|   20|             1.0|   (3,[1],[1.0])|\n",
      "|         B|   30|             0.0|   (3,[0],[1.0])|\n",
      "|         B|   20|             0.0|   (3,[0],[1.0])|\n",
      "|         B|   30|             0.0|   (3,[0],[1.0])|\n",
      "|         C|   40|             2.0|   (3,[2],[1.0])|\n",
      "|         C|   10|             2.0|   (3,[2],[1.0])|\n",
      "|         D|   10|             3.0|       (3,[],[])|\n",
      "+----------+-----+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", 10),(\"A\", 20),(\"B\", 30),(\"B\", 20),(\"B\", 30),(\"C\", 40),(\"C\", 10),(\"D\", 10)]\n",
    "columns = [\"Categories\", \"Value\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "#before performing one Hot Encoding  perform Sting Intexer so that the string will be assigned values.\n",
    "indexer=StringIndexer(inputCol=\"Categories\",outputCol=\"category_numeric\")\n",
    "indexer_fitted=indexer.fit(df)\n",
    "df_indexed=indexer_fitted.transform(df)\n",
    "df_indexed.show()\n",
    "encoder=OneHotEncoder(inputCols=[\"category_numeric\"],outputCols=[\"category_encoded\"])\n",
    "df_onehot=encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_onehot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
